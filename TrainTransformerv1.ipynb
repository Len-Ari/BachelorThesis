{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b512e60d-c718-45c3-80a6-e45801f53fbd",
   "metadata": {},
   "source": [
    "# Train Transformer Models V1\n",
    "\n",
    "Used to train transformer models by creating own metric for similarity between abstracts and using those to backpropagate for abstract-title pairs.\n",
    "\n",
    "Simply use a multiple negative triplet loss and train it, giving only positive pairs beeing title-abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606ed955-ebd8-44c2-a00f-ef968f33c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abecker/.conda/envs/BachelorEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "import save_load_models as save_load\n",
    "import vector_search as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc8793b-7ecb-4c72-8a73-57ffd49240e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(save_load)\n",
    "\n",
    "sPubMedBert = './Models/SPubMedBERT'\n",
    "\n",
    "model = save_load.load_model_sentencetransformer(sPubMedBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e34705-48d6-4932-83b6-66ef3cb059f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './Models/Finetunedv1/SPubMedBERT/'\n",
    "\n",
    "# Train_Data_Subset\n",
    "# Train_Data_10k\n",
    "dataDir = './Data/Train_Data_Subset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf499ec-63ab-4604-9ca9-96acce7fbff9",
   "metadata": {},
   "source": [
    "## Training-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c42094c-a478-4a93-b7ad-36c04ae44730",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vs)\n",
    "\n",
    "# Get Text passages from subset\n",
    "text_passages, ref_list = vs.get_title_abstract_from_dir(dataDir)\n",
    "# Take only title text\n",
    "title_text, title_text_ref = vs.filter_embedding_types(text_passages, ref_list, embedding_types=['Title'])\n",
    "# Take only abstract text\n",
    "abstract_text, abstract_text_ref = vs.filter_embedding_types(text_passages, ref_list, embedding_types=['Abstract'])\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"anchor\": title_text,\n",
    "    \"positive\": abstract_text,\n",
    "})\n",
    "\n",
    "# Index of end of train\n",
    "idx_1 = int(len(dataset)*0.8)\n",
    "idx_2 = int(len(dataset)*0.9)\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": title_text[:idx_1],\n",
    "    \"positive\": abstract_text[:idx_1],\n",
    "})\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"anchor\": title_text[idx_1:idx_2],\n",
    "    \"positive\": abstract_text[idx_1:idx_2],\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"anchor\": title_text[idx_2:],\n",
    "    \"positive\": abstract_text[idx_2:],\n",
    "})\n",
    "'''\n",
    "'''\n",
    "train_dataset = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021feb1e-dc45-4096-a531-09b44dee02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['anchor', 'positive'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8f0f4-1b34-4f55-a184-fcaeb649c65d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Eval/Test-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efe8828b-ca84-4576-b1f1-fee7b90442a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'score'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"tabilab/biosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e7f29fa-5b52-439f-bd4c-b59632a54f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.200000047683716\n",
      "[0.550000011920929, 0.800000011920929, 0.5, 0.699999988079071, 0.6000000238418579, 0.75, 0.05000000074505806, 1.0, 0.75, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "def divide_score(example):\n",
    "    example['score'] = example['score'] / 4\n",
    "    return example\n",
    "print(eval_dataset['train'][0]['score'])\n",
    "new_dataset = eval_dataset.map(divide_score)\n",
    "print(new_dataset['train'][0:10]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e543f19-80f7-49ae-a890-dd1ae772b831",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdce46d-1282-44d3-bb4a-7f4b393588d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 08:07, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers.losses import MultipleNegativesSymmetricRankingLoss\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "\n",
    "loss = MultipleNegativesSymmetricRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay= 0.01,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=300,\n",
    ")\n",
    "args = args.set_dataloader(train_batch_size=32)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "if not os.path.isdir(save_dir+\"/end_model\"):\n",
    "        os.makedirs(save_dir+\"/end_model\")\n",
    "model.save_pretrained(save_dir+\"/end_model\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b406f644-7c49-4445-91c8-cb7f88d369d7",
   "metadata": {},
   "source": [
    "0.018200\n",
    "2 Epochs ~ 24 minutes\n",
    "0.029400\n",
    "2 Epochs ~ 12 minutes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2c9c577-c07a-4114-8833-e0646a7a68ec",
   "metadata": {},
   "source": [
    "MiniLM ~ 34 min\n",
    "MsMarco ~ 48 min\n",
    "Wikimedical ~ 70 min\n",
    "SPubMedBert ~ 70 min"
   ]
  },
  {
   "cell_type": "raw",
   "id": "528d23c1-b52f-41e2-a2c9-c3b3803c1518",
   "metadata": {},
   "source": [
    "test1:\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "----------------------\n",
    "test2:\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "----------------------\n",
    "test3:\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay= 0.01,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=1000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
