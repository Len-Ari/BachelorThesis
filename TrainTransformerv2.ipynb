{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990150f2-ef28-46d1-8338-4d9e75db2c22",
   "metadata": {},
   "source": [
    "# Train Transformer Models V2\n",
    "\n",
    "Used to train transformer models by creating own metric for similarity between abstracts and using those to backpropagate for abstract-title pairs.\n",
    "\n",
    "For now: Given a certain abstract: Use title with similarity close to 1. \n",
    "Find 2-3 other abstracts (either randomly or 1 close 1 middle 1 far) and take their similarity as goal between the titles of given abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faf3426-e53c-4c8b-88e7-8d3cd038bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abecker/.conda/envs/BachelorEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import save_load_models as save_load\n",
    "import vector_search as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c705d59-0ae4-4bb9-a890-5967ee07025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(save_load)\n",
    "\n",
    "model_finetune = save_load.load_model_sentencetransformer('./Models/SPubMedBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3844599-d081-4c9e-bd32-4b5f7462410e",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "Using the data already existant trying to determine similarities for model.\n",
    "For now using same model as goal (might not do much... But is easier as similaries in same numeric area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8e1669-0875-4150-ac6e-546b88819d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(vs)\n",
    "\n",
    "# Use embeddings from same model...\n",
    "# Train_Data_Subset\n",
    "# Train_Data_10k\n",
    "embeddings_label, text_label, ref_list_label = vs.add_embeddings_and_text_to_nparray_from_dir('./Embeddings/SPubMedBERT/Train_Data_Subset/')\n",
    "# Take only title text\n",
    "title_text, title_text_ref = vs.filter_embedding_types(text_label, ref_list_label, embedding_types=['Title'])\n",
    "# Take only abstract text\n",
    "abstract_text, abstract_text_ref = vs.filter_embedding_types(text_label, ref_list_label, embedding_types=['Abstract'])\n",
    "# Take only abstract embeddings\n",
    "embeddings_abstr, ref_list_abstr = vs.filter_embedding_types(embeddings_label, ref_list_label, embedding_types=['Abstract'])\n",
    "# Calculate similarity between abstract embeddings\n",
    "all_similarity = vs.calc_all_embedding_similarity(embeddings_abstr, scaled=False)\n",
    "\n",
    "\n",
    "n = 3\n",
    "titles = []\n",
    "abstracts = []\n",
    "sim_scores = []\n",
    "\n",
    "# Loop through abstracts\n",
    "for abstr_idx in range(all_similarity.shape[0]):\n",
    "    # Add title-abstract at abstr_idx with similarity 1\n",
    "    titles.append(title_text[abstr_idx])\n",
    "    abstracts.append(abstract_text[abstr_idx])\n",
    "    # Could just use 1 or 0.999 as sim_score here\n",
    "    sim_scores.append(all_similarity[abstr_idx][abstr_idx])\n",
    "    # Sample n abstracts and add their title with all_similarity[idx]\n",
    "    for i in range(n):\n",
    "        rand_idx = random.randint(0, all_similarity.shape[0]-2)\n",
    "        # Make sure rand_idx != abstr_idx\n",
    "        rand_idx += (int)(rand_idx >= abstr_idx)\n",
    "        titles.append(title_text[rand_idx])\n",
    "        abstracts.append(abstract_text[abstr_idx])\n",
    "        sim_scores.append(all_similarity[abstr_idx][rand_idx])\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"sentence_A\": titles,\n",
    "    \"sentence_B\": abstracts,\n",
    "    \"label\": sim_scores,\n",
    "})\n",
    "# Index of end of train\n",
    "idx_1 = int(len(dataset)*0.8)\n",
    "idx_2 = int(len(dataset)*0.9)\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence_A\": titles[:idx_1],\n",
    "    \"sentence_B\": abstracts[:idx_1],\n",
    "    \"label\": sim_scores[:idx_1],\n",
    "})\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"sentence_A\": titles[idx_1:idx_2],\n",
    "    \"sentence_B\": abstracts[idx_1:idx_2],\n",
    "    \"label\": sim_scores[idx_1:idx_2],\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"sentence_A\": titles[idx_2:],\n",
    "    \"sentence_B\": abstracts[idx_2:],\n",
    "    \"label\": sim_scores[idx_2:],\n",
    "})\n",
    "train_dataset = train_dataset.shuffle()\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adf8f19-8f2d-4220-a8f4-6d585f7204c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence_A', 'sentence_B', 'label'],\n",
      "    num_rows: 75836\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f96535-cb08-413a-8525-d9e66fa52d1e",
   "metadata": {},
   "source": [
    "## Implement Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccff0efa-e6d5-4e5f-9866-5ef8d1cc6d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4740' max='4740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4740/4740 22:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.784600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=save_dir+\"/checkpoints\",\\n    num_train_epochs=2,\\n    learning_rate=2e-5,\\n    adam_epsilon=5e-06,\\n    warmup_ratio=0.1,\\n    weight_decay= 0.01,\\n\\n    \\n    eval_strategy=\"steps\",\\n    eval_steps=1000,\\n    save_strategy=\"epoch\",\\n    save_total_limit=1,\\n    logging_steps=100,\\n)\\n\\ndev_evaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=eval_dataset[\"sentence_A\"],\\n    sentences2=eval_dataset[\"sentence_B\"],\\n    scores=eval_dataset[\"label\"],\\n    main_similarity=SimilarityFunction.COSINE,\\n)\\ndev_evaluator(model_finetune)\\n\\ntrainer = SentenceTransformerTrainer(\\n    model=model_finetune,\\n    args=args,\\n    train_dataset=train_dataset,\\n    eval_dataset=eval_dataset,\\n    loss=loss,\\n    evaluator=dev_evaluator,\\n)\\ntrainer.train()\\n\\n# Evaluate the trained model on the test set\\ntest_evaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=eval_dataset[\"sentence_A\"],\\n    sentences2=eval_dataset[\"sentence_B\"],\\n    scores=eval_dataset[\"label\"],\\n    main_similarity=SimilarityFunction.COSINE,\\n)\\ntest_evaluator(model_finetune)\\n\\n# Save the trained model\\nif not os.path.isdir(save_dir+\"/end_model\"):\\n        os.makedirs(save_dir+\"/end_model\")\\nmodel_finetune.save_pretrained(save_dir+\"/end_model\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers.losses import CosineSimilarityLoss, CoSENTLoss\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "\n",
    "save_dir = './Models/Finetunedv2/SPubMedBERT/'\n",
    "\n",
    "#loss = CosineSimilarityLoss(model_finetune)\n",
    "loss = CoSENTLoss(model_finetune)\n",
    "\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay= 0.01,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=500,\n",
    ")\n",
    "args = args.set_dataloader(train_batch_size=16)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model_finetune,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "if not os.path.isdir(save_dir+\"/end_model\"):\n",
    "        os.makedirs(save_dir+\"/end_model\")\n",
    "model_finetune.save_pretrained(save_dir+\"/end_model\")\n",
    "\n",
    "\"\"\"\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_dir+\"/checkpoints\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    adam_epsilon=5e-06,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay= 0.01,\n",
    "\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence_A\"],\n",
    "    sentences2=eval_dataset[\"sentence_B\"],\n",
    "    scores=eval_dataset[\"label\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    ")\n",
    "dev_evaluator(model_finetune)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model_finetune,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=eval_dataset[\"sentence_A\"],\n",
    "    sentences2=eval_dataset[\"sentence_B\"],\n",
    "    scores=eval_dataset[\"label\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    ")\n",
    "test_evaluator(model_finetune)\n",
    "\n",
    "# Save the trained model\n",
    "if not os.path.isdir(save_dir+\"/end_model\"):\n",
    "        os.makedirs(save_dir+\"/end_model\")\n",
    "model_finetune.save_pretrained(save_dir+\"/end_model\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad056a23-3b3b-4c07-99ff-1ceb2dc352ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
